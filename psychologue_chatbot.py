import streamlit as st
import os
from pathlib import Path
import re
from typing import List, Dict, Tuple
import json
from datetime import datetime

# Import Ollama utilities (version simplifi√©e)
try:
    from ollama_utils_simple import get_available_models, get_chat_response
except ImportError:
    st.error("Erreur: Impossible d'importer ollama_utils_simple. Assurez-vous que le fichier existe.")
    st.stop()

class PsychologueChatbot:
    def __init__(self, data_dir: str = "psychologie_data"):
        self.data_dir = Path(data_dir)
        self.knowledge_base = self.load_knowledge_base()

    def load_knowledge_base(self) -> Dict[str, str]:
        """Charge la base de connaissances depuis les fichiers Markdown"""
        knowledge = {}

        if not self.data_dir.exists():
            return knowledge

        for file_path in self.data_dir.glob("*.md"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # Extraire le titre du fichier
                    title = file_path.stem.replace('_', ' ').title()
                    knowledge[title] = content
            except Exception as e:
                st.warning(f"Erreur lors du chargement de {file_path}: {e}")

        return knowledge

    def search_knowledge(self, query: str, max_results: int = 3) -> List[Tuple[str, str, float]]:
        """Recherche dans la base de connaissances avec scoring de pertinence"""
        results = []

        # Tokenization simple de la requ√™te
        query_words = set(re.findall(r'\b\w+\b', query.lower()))

        for title, content in self.knowledge_base.items():
            # Recherche dans le titre
            title_score = len(query_words.intersection(set(re.findall(r'\b\w+\b', title.lower()))))

            # Recherche dans le contenu
            content_words = set(re.findall(r'\b\w+\b', content.lower()))
            content_score = len(query_words.intersection(content_words))

            # Score total (titre plus important)
            total_score = title_score * 2 + content_score

            if total_score > 0:
                # Extraire un extrait pertinent autour des mots-cl√©s trouv√©s
                excerpt = self.extract_relevant_excerpt(content, query_words)
                results.append((title, excerpt, total_score))

        # Trier par score d√©croissant et limiter les r√©sultats
        results.sort(key=lambda x: x[2], reverse=True)
        return results[:max_results]

    def extract_relevant_excerpt(self, content: str, query_words: set, context_chars: int = 200) -> str:
        """Extrait un passage pertinent du contenu autour des mots-cl√©s"""
        content_lower = content.lower()

        # Trouver la premi√®re occurrence d'un mot-cl√©
        best_pos = len(content)
        for word in query_words:
            pos = content_lower.find(word)
            if pos != -1 and pos < best_pos:
                best_pos = pos

        if best_pos == len(content):
            # Aucun mot-cl√© trouv√©, retourner le d√©but du contenu
            return content[:context_chars] + "..."

        # Extraire autour de la position trouv√©e
        start = max(0, best_pos - context_chars // 2)
        end = min(len(content), best_pos + context_chars // 2)

        excerpt = content[start:end]
        if start > 0:
            excerpt = "..." + excerpt
        if end < len(content):
            excerpt = excerpt + "..."

        return excerpt

    def generate_response(self, user_message: str, model_name: str, conversation_history: List[Dict] = None) -> str:
        """G√©n√®re une r√©ponse en utilisant RAG + LLM"""

        # Recherche dans la base de connaissances
        relevant_docs = self.search_knowledge(user_message)

        # Construire le contexte avec les documents pertinents
        context = ""
        if relevant_docs:
            context = "\n\n".join([
                f"üìö Information pertinente - {title}:\n{excerpt}"
                for title, excerpt, score in relevant_docs
            ])

        # Prompt syst√®me pour le psychologue
        system_prompt = """Tu es un psychologue clinicien empathique et professionnel.
Tu dois :
- √âcouter activement et montrer de l'empathie
- Utiliser les connaissances psychologiques fournies quand c'est pertinent
- √âviter de donner des diagnostics m√©dicaux d√©finitifs
- Encourager l'utilisateur √† consulter un professionnel si n√©cessaire
- R√©pondre en fran√ßais de mani√®re naturelle et bienveillante
- Ne jamais remplacer un suivi th√©rapeutique professionnel

Connaissances disponibles :
{context}

Si tu n'as pas assez d'informations sp√©cifiques, utilise tes connaissances g√©n√©rales en psychologie."""

        # Historique de conversation
        if conversation_history is None:
            conversation_history = []

        # Ajouter le message utilisateur √† l'historique
        conversation_history.append({"role": "user", "content": user_message})

        # Construire le prompt avec contexte
        full_prompt = f"""Contexte psychologique pertinent :
{context}

Question de l'utilisateur : {user_message}

R√©ponds en tant que psychologue professionnel, utilisant les informations ci-dessus si elles sont pertinentes."""

        try:
            # Utiliser Ollama pour g√©n√©rer la r√©ponse
            response = get_chat_response(
                model_name=model_name,
                user_message=full_prompt,
                system_prompt=system_prompt
            )

            # Ajouter la r√©ponse √† l'historique
            conversation_history.append({"role": "assistant", "content": response})

            return response

        except Exception as e:
            return f"Erreur lors de la g√©n√©ration de la r√©ponse : {e}"

def main():
    st.set_page_config(
        page_title="Chatbot Psychologue ü§ó",
        page_icon="üß†",
        layout="wide"
    )

    st.title("üß† Chatbot Psychologue")
    st.markdown("*Un assistant IA pour l'√©coute et le soutien psychologique*")

    # Initialiser le chatbot
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = PsychologueChatbot()

    if 'conversation_history' not in st.session_state:
        st.session_state.conversation_history = []

    if 'current_model' not in st.session_state:
        st.session_state.current_model = None

    # Sidebar pour la configuration
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")

        # S√©lection du mod√®le
        models = get_available_models()
        if models:
            selected_model = st.selectbox(
                "Mod√®le Ollama :",
                models,
                index=models.index(st.session_state.current_model) if st.session_state.current_model in models else 0
            )
            st.session_state.current_model = selected_model
        else:
            st.error("Aucun mod√®le Ollama disponible. Lancez Ollama et installez un mod√®le.")
            return

        # Informations sur la base de connaissances
        st.header("üìö Base de connaissances")
        num_docs = len(st.session_state.chatbot.knowledge_base)
        st.info(f"{num_docs} documents charg√©s")

        if st.button("üîÑ Recharger la base"):
            st.session_state.chatbot = PsychologueChatbot()
            st.success("Base de connaissances recharg√©e !")

        # Bouton pour effacer l'historique
        if st.button("üóëÔ∏è Effacer la conversation"):
            st.session_state.conversation_history = []
            st.success("Conversation effac√©e !")

    # Zone principale de chat
    st.header("üí¨ Discussion")

    # Afficher l'historique des messages
    chat_container = st.container()
    with chat_container:
        for message in st.session_state.conversation_history:
            if message["role"] == "user":
                with st.chat_message("user"):
                    st.write(message["content"])
            else:
                with st.chat_message("assistant"):
                    st.write(message["content"])

    # Input pour nouveau message
    if prompt := st.chat_input("Partagez ce qui vous pr√©occupe..."):
        if not st.session_state.current_model:
            st.error("Veuillez s√©lectionner un mod√®le dans la sidebar.")
            return

        # Ajouter le message utilisateur
        with chat_container:
            with st.chat_message("user"):
                st.write(prompt)

        # G√©n√©rer la r√©ponse
        with chat_container:
            with st.chat_message("assistant"):
                with st.spinner("Le psychologue r√©fl√©chit..."):
                    response = st.session_state.chatbot.generate_response(
                        prompt,
                        st.session_state.current_model,
                        st.session_state.conversation_history
                    )
                st.write(response)

        # Ajouter √† l'historique
        st.session_state.conversation_history.append({"role": "user", "content": prompt})
        st.session_state.conversation_history.append({"role": "assistant", "content": response})

        # Scroll automatique vers le bas
        st.rerun()

    # Footer avec disclaimer
    st.markdown("---")
    st.caption("""
    ‚ö†Ô∏è **Disclaimer important** : Ce chatbot est un outil d'information g√©n√©rale et ne remplace pas une consultation avec un professionnel de sant√© qualifi√©.
    En cas de d√©tresse psychologique importante, contactez un psychologue ou un service d'urgence.
    """)

if __name__ == "__main__":
    main()