import streamlit as st
import os
from pathlib import Path
import re
from typing import List, Dict, Tuple
import json
from datetime import datetime

# Import Ollama utilities (version simplifiÃ©e)
try:
    from ollama_utils_simple import get_available_models, get_chat_response
except ImportError:
    st.error("Erreur: Impossible d'importer ollama_utils_simple. Assurez-vous que le fichier existe.")
    st.stop()

class PsychologueChatbot:
    def __init__(self, data_dir: str = "psychologie_data"):
        self.data_dir = Path(data_dir)
        self.knowledge_base = self.load_knowledge_base()

    def load_knowledge_base(self) -> Dict[str, str]:
        """Charge la base de connaissances depuis les fichiers Markdown"""
        knowledge = {}

        if not self.data_dir.exists():
            return knowledge

        for file_path in self.data_dir.glob("*.md"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # Extraire le titre du fichier
                    title = file_path.stem.replace('_', ' ').title()
                    knowledge[title] = content
            except Exception as e:
                st.warning(f"Erreur lors du chargement de {file_path}: {e}")

        return knowledge

    def search_knowledge(self, query: str, max_results: int = 3) -> List[Tuple[str, str, float]]:
        """Recherche dans la base de connaissances avec scoring de pertinence"""
        results = []

        # Tokenization simple de la requÃªte
        query_words = set(re.findall(r'\b\w+\b', query.lower()))

        for title, content in self.knowledge_base.items():
            # Recherche dans le titre
            title_score = len(query_words.intersection(set(re.findall(r'\b\w+\b', title.lower()))))

            # Recherche dans le contenu
            content_words = set(re.findall(r'\b\w+\b', content.lower()))
            content_score = len(query_words.intersection(content_words))

            # Score total (titre plus important)
            total_score = title_score * 2 + content_score

            if total_score > 0:
                # Extraire un extrait pertinent autour des mots-clÃ©s trouvÃ©s
                excerpt = self.extract_relevant_excerpt(content, query_words)
                results.append((title, excerpt, total_score))

        # Trier par score dÃ©croissant et limiter les rÃ©sultats
        results.sort(key=lambda x: x[2], reverse=True)
        return results[:max_results]

    def extract_relevant_excerpt(self, content: str, query_words: set, context_chars: int = 200) -> str:
        """Extrait un passage pertinent du contenu autour des mots-clÃ©s"""
        content_lower = content.lower()

        # Trouver la premiÃ¨re occurrence d'un mot-clÃ©
        best_pos = len(content)
        for word in query_words:
            pos = content_lower.find(word)
            if pos != -1 and pos < best_pos:
                best_pos = pos

        if best_pos == len(content):
            # Aucun mot-clÃ© trouvÃ©, retourner le dÃ©but du contenu
            return content[:context_chars] + "..."

        # Extraire autour de la position trouvÃ©e
        start = max(0, best_pos - context_chars // 2)
        end = min(len(content), best_pos + context_chars // 2)

        excerpt = content[start:end]
        if start > 0:
            excerpt = "..." + excerpt
        if end < len(content):
            excerpt = excerpt + "..."

        return excerpt

    def generate_response(self, user_message: str, model_name: str, conversation_history: List[Dict] = None) -> str:
        """GÃ©nÃ¨re une rÃ©ponse en utilisant RAG + LLM"""

        # Recherche dans la base de connaissances
        relevant_docs = self.search_knowledge(user_message)

        # Construire le contexte avec les documents pertinents
        context = ""
        if relevant_docs:
            context = "\n\n".join([
                f"ğŸ“š Information pertinente - {title}:\n{excerpt}"
                for title, excerpt, score in relevant_docs
            ])

        # Prompt systÃ¨me pour le psychologue
        system_prompt = """Tu es un psychologue clinicien empathique et professionnel.
Tu dois :
- Ã‰couter activement et montrer de l'empathie
- Utiliser les connaissances psychologiques fournies quand c'est pertinent
- Ã‰viter de donner des diagnostics mÃ©dicaux dÃ©finitifs
- Encourager l'utilisateur Ã  consulter un professionnel si nÃ©cessaire
- RÃ©pondre en franÃ§ais de maniÃ¨re naturelle et bienveillante
- Ne jamais remplacer un suivi thÃ©rapeutique professionnel

Connaissances disponibles :
{context}

Si tu n'as pas assez d'informations spÃ©cifiques, utilise tes connaissances gÃ©nÃ©rales en psychologie."""

        # Historique de conversation
        if conversation_history is None:
            conversation_history = []

        # Ajouter le message utilisateur Ã  l'historique
        conversation_history.append({"role": "user", "content": user_message})

        # Construire le prompt avec contexte
        full_prompt = f"""Contexte psychologique pertinent :
{context}

Question de l'utilisateur : {user_message}

RÃ©ponds en tant que psychologue professionnel, utilisant les informations ci-dessus si elles sont pertinentes."""

        try:
            # Utiliser Ollama pour gÃ©nÃ©rer la rÃ©ponse
            response = get_chat_response(
                model_name=model_name,
                user_message=full_prompt,
                system_prompt=system_prompt
            )

            # Ajouter la rÃ©ponse Ã  l'historique
            conversation_history.append({"role": "assistant", "content": response})

            return response

        except Exception as e:
            return f"Erreur lors de la gÃ©nÃ©ration de la rÃ©ponse : {e}"

def main():
    st.set_page_config(
        page_title="Chatbot Psychologue ğŸ¤—",
        page_icon="ğŸ§ ",
        layout="wide"
    )

    st.title("ğŸ§  Chatbot Psychologue")
    st.markdown("*Un assistant IA pour l'Ã©coute et le soutien psychologique*")

    # AVERTISSEMENT CRITIQUE
    st.error("ğŸš¨ **AVERTISSEMENT CRITIQUE** ğŸš¨")
    st.markdown("""
    **CE CHATBOT EST UN EXEMPLE TECHNIQUE DE RAG UNIQUEMENT**

    - âŒ **PAS de diagnostic mÃ©dical** : Ne peut pas diagnostiquer de troubles mentaux
    - âŒ **PAS de traitement** : Ne remplace pas une thÃ©rapie professionnelle
    - âŒ **PAS d'urgence** : En cas de dÃ©tresse, contactez immÃ©diatement un professionnel

    âœ… **C'est un exemple Ã©ducatif** qui peut Ãªtre adaptÃ© Ã  d'autres domaines (Ã©ducation, RH, documentation)
    """)

    # Initialiser le chatbot
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = PsychologueChatbot()

    if 'conversation_history' not in st.session_state:
        st.session_state.conversation_history = []

    if 'current_model' not in st.session_state:
        st.session_state.current_model = None

    # Sidebar pour la configuration
    with st.sidebar:
        st.header("âš™ï¸ Configuration")

        # SÃ©lection du modÃ¨le
        models = get_available_models()
        if models:
            selected_model = st.selectbox(
                "ModÃ¨le Ollama :",
                models,
                index=models.index(st.session_state.current_model) if st.session_state.current_model in models else 0
            )
            st.session_state.current_model = selected_model
        else:
            st.error("Aucun modÃ¨le Ollama disponible. Lancez Ollama et installez un modÃ¨le.")
            return

        # Informations sur la base de connaissances
        st.header("ğŸ“š Base de connaissances")
        num_docs = len(st.session_state.chatbot.knowledge_base)
        st.info(f"{num_docs} documents chargÃ©s")

        if st.button("ğŸ”„ Recharger la base"):
            st.session_state.chatbot = PsychologueChatbot()
            st.success("Base de connaissances rechargÃ©e !")

        # Bouton pour effacer l'historique
        if st.button("ğŸ—‘ï¸ Effacer la conversation"):
            st.session_state.conversation_history = []
            st.success("Conversation effacÃ©e !")

    # Zone principale de chat
    st.header("ğŸ’¬ Discussion")

    # Afficher l'historique des messages
    chat_container = st.container()
    with chat_container:
        for message in st.session_state.conversation_history:
            if message["role"] == "user":
                with st.chat_message("user"):
                    st.write(message["content"])
            else:
                with st.chat_message("assistant"):
                    st.write(message["content"])

    # Input pour nouveau message
    if prompt := st.chat_input("Partagez ce qui vous prÃ©occupe..."):
        if not st.session_state.current_model:
            st.error("Veuillez sÃ©lectionner un modÃ¨le dans la sidebar.")
            return

        # Ajouter le message utilisateur
        with chat_container:
            with st.chat_message("user"):
                st.write(prompt)

        # GÃ©nÃ©rer la rÃ©ponse
        with chat_container:
            with st.chat_message("assistant"):
                with st.spinner("Le psychologue rÃ©flÃ©chit..."):
                    response = st.session_state.chatbot.generate_response(
                        prompt,
                        st.session_state.current_model,
                        st.session_state.conversation_history
                    )
                st.write(response)

        # Ajouter Ã  l'historique
        st.session_state.conversation_history.append({"role": "user", "content": prompt})
        st.session_state.conversation_history.append({"role": "assistant", "content": response})

        # Scroll automatique vers le bas
        st.rerun()

    # Footer avec disclaimer renforcÃ©
    st.markdown("---")
    st.error("ğŸš¨ **RAPPEL CRITIQUE** ğŸš¨")
    st.markdown("""
    **CE CHATBOT EST UN EXEMPLE TECHNIQUE DE RAG - PAS UN OUTIL MÃ‰DICAL**

    - ğŸ©º **Jamais de diagnostic** : L'IA ne peut pas remplacer un professionnel de santÃ©
    - ğŸ’Š **Jamais de traitement** : Les rÃ©ponses sont informatives uniquement
    - ğŸš‘ **En cas d'urgence** : Contactez immÃ©diatement les services appropriÃ©s

    **Ressources d'aide professionnelles :**
    - ğŸ‡«ğŸ‡· **SAMU** : 15 (urgences mÃ©dicales)
    - ğŸ‡«ğŸ‡· **SOS MÃ©decins** : 3624
    - ğŸ‡«ğŸ‡· **SOS AmitiÃ©** : 09 72 39 40 50 (Ã©coute 24h/24)
    - ğŸ‡«ğŸ‡· **Fil SantÃ© Jeunes** : 0 800 235 236 (3-25 ans)
    - ğŸŒ **Votre mÃ©decin traitant** ou **psychologue**

    *Ce projet est un exemple Ã©ducatif qui peut Ãªtre adaptÃ© Ã  d'autres domaines.*
    """)

if __name__ == "__main__":
    main()