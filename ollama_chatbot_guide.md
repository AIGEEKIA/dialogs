# Ollama - Ex√©cution locale de mod√®les de langage

## Qu'est-ce qu'Ollama ?

Ollama est une plateforme open-source qui permet d'ex√©cuter des mod√®les de langage (LLM) localement sur votre machine. Contrairement aux APIs cloud comme OpenAI ou Anthropic, Ollama offre :

- **Confidentialit√© totale** : Vos donn√©es restent sur votre machine
- **Pas de co√ªts** : Utilisation gratuite apr√®s t√©l√©chargement initial
- **Hors ligne** : Fonctionne sans connexion internet
- **Performance optimis√©e** : Mod√®les quantifi√©s pour un usage efficace

## Installation et premiers pas

### Installation

```bash
# macOS/Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# T√©l√©charger depuis https://ollama.ai/download
```

### T√©l√©chargement d'un mod√®le

```bash
# Mod√®le de base (recommand√© pour d√©buter)
ollama pull llama2

# Autres mod√®les populaires
ollama pull mistral
ollama pull codellama
ollama pull llama2:13b
ollama pull llama2:70b
```

### V√©rification de l'installation

```bash
ollama list  # Voir les mod√®les install√©s
ollama serve # D√©marrer le serveur (optionnel)
```

## Architecture et fonctionnement

Ollama fonctionne comme un serveur local qui expose une API REST similaire √† OpenAI. Les mod√®les sont automatiquement quantifi√©s et optimis√©s pour votre mat√©riel.

### API REST

```bash
# Test rapide
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Bonjour, comment allez-vous ?"
}'
```

### Int√©gration Python

```python
import requests

response = requests.post('http://localhost:11434/api/generate', json={
    'model': 'llama2',
    'prompt': 'Expliquez ce qu\'est l\'intelligence artificielle'
})

print(response.json()['response'])
```

## Cr√©er un chatbot Streamlit avec Ollama

### Application minimale (chatbot simple)

Voici le code le plus simple possible pour cr√©er un chatbot Streamlit avec Ollama :

```python
import streamlit as st
import requests

# Configuration
OLLAMA_URL = "http://localhost:11434/api/generate"

st.title("ü§ñ Chatbot Ollama")

# S√©lection du mod√®le
models = ["llama2", "mistral", "codellama"]  # Mod√®les disponibles
selected_model = st.selectbox("Choisir un mod√®le :", models)

# Zone de chat
if "messages" not in st.session_state:
    st.session_state.messages = []

# Afficher l'historique
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Input utilisateur
if prompt := st.chat_input("Votre message..."):
    # Ajouter le message utilisateur
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # G√©n√©rer la r√©ponse
    with st.chat_message("assistant"):
        with st.spinner("R√©flexion..."):
            try:
                response = requests.post(OLLAMA_URL, json={
                    "model": selected_model,
                    "prompt": prompt,
                    "stream": False
                })

                if response.status_code == 200:
                    result = response.json()
                    reply = result.get("response", "Erreur dans la r√©ponse")
                    st.markdown(reply)
                    st.session_state.messages.append({"role": "assistant", "content": reply})
                else:
                    st.error(f"Erreur HTTP: {response.status_code}")

            except Exception as e:
                st.error(f"Erreur de connexion: {e}")
                st.info("V√©rifiez qu'Ollama est d√©marr√© : `ollama serve`")
```

### Fonctionnalit√©s de cette application

- ‚úÖ **S√©lection de mod√®le** : Liste d√©roulante pour choisir le LLM
- ‚úÖ **Interface chat** : Messages altern√©s utilisateur/assistant
- ‚úÖ **Historique** : Conservation des messages pendant la session
- ‚úÖ **Gestion d'erreurs** : Messages informatifs en cas de probl√®me
- ‚úÖ **Interface moderne** : Utilise les nouveaux composants Streamlit

### Am√©liorations possibles

#### 1. Liste dynamique des mod√®les

```python
def get_available_models():
    try:
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            models = response.json().get("models", [])
            return [model["name"] for model in models]
    except:
        return ["llama2"]  # Mod√®le par d√©faut

# Dans l'interface
models = get_available_models()
selected_model = st.selectbox("Choisir un mod√®le :", models)
```

#### 2. Param√®tres avanc√©s

```python
st.sidebar.header("Param√®tres")

temperature = st.sidebar.slider("Temp√©rature", 0.0, 2.0, 0.7)
max_tokens = st.sidebar.slider("Tokens max", 50, 2000, 500)

# Dans la requ√™te
response = requests.post(OLLAMA_URL, json={
    "model": selected_model,
    "prompt": prompt,
    "options": {
        "temperature": temperature,
        "num_predict": max_tokens
    }
})
```

#### 3. Streaming en temps r√©el

```python
# Pour un affichage progressif
response = requests.post(OLLAMA_URL, json={
    "model": selected_model,
    "prompt": prompt,
    "stream": True
}, stream=True)

placeholder = st.empty()
full_response = ""

for line in response.iter_lines():
    if line:
        chunk = json.loads(line.decode('utf-8'))
        if chunk.get("done"):
            break
        token = chunk.get("response", "")
        full_response += token
        placeholder.markdown(full_response + "‚ñå")

placeholder.markdown(full_response)
```

## Mod√®les recommand√©s

### Pour le chat g√©n√©ral

- **llama2** : Mod√®le √©quilibr√©, bon pour la conversation
- **mistral** : Excellent pour le raisonnement, plus concis
- **vicuna** : Sp√©cialis√© dans les conversations naturelles

### Pour la programmation

- **codellama** : Expert en code, g√©n√©ration et explication
- **deepseek-coder** : Sp√©cialis√© dans les langages de programmation

### Pour l'analyse de donn√©es

- **llama2:13b** : Bonne capacit√© d'analyse
- **mistral** : Bon pour les t√¢ches structur√©es

## D√©pannage courant

### "Connection refused"

```bash
# V√©rifier qu'Ollama tourne
ollama serve

# Ou v√©rifier le port
netstat -an | grep 11434
```

### "Model not found"

```bash
# Lister les mod√®les disponibles
ollama list

# T√©l√©charger un mod√®le
ollama pull llama2
```

### Performance lente

- Utilisez des mod√®les plus petits (`llama2:7b` au lieu de `llama2:70b`)
- Activez l'acc√©l√©ration GPU si disponible
- Fermez les autres applications gourmandes en RAM

## Ressources et communaut√©

### Documentation officielle

- [Site web Ollama](https://ollama.ai/)
- [Mod√®les disponibles](https://ollama.ai/library)
- [Guide d'installation](https://github.com/jmorganca/ollama)

### Communaut√©

- **Discord Ollama** : Support et discussions
- **Reddit r/ollama** : Partage d'exp√©riences
- **GitHub Issues** : Signalement de bugs

### Alternatives

- **LM Studio** : Interface graphique pour mod√®les locaux
- **GPT4All** : Alternative open-source
- **LocalAI** : API compatible OpenAI pour mod√®les locaux

Ollama d√©mocratise l'acc√®s aux LLM en permettant leur ex√©cution locale, offrant confidentialit√© et contr√¥le total sur vos donn√©es tout en maintenant des performances impressionnantes.
