# ğŸ­ Assistant de Dialogues avec Ollama

Une application de surveillance de dialogues qui utilise l'IA pour assister Ã  la gestion des rÃ©ponses dans des conversations. L'application surveille un dossier de fichiers de dialogue et permet de gÃ©nÃ©rer des rÃ©ponses contextuelles pour diffÃ©rents personnages avec un contrÃ´le prÃ©cis du comportement du LLM.

## ğŸ¯ Objectif Principal

Cette application est conÃ§ue pour **assister les crÃ©ateurs de contenu, scÃ©naristes, auteurs et dÃ©veloppeurs de chatbots** dans la gestion et la continuation de dialogues complexes. Elle surveille un dossier de fichiers de dialogue et offre une interface intelligente pour :

- ğŸ‘¥ **Choisir le personnage** qui doit rÃ©pondre
- ğŸ¨ **ContrÃ´ler le comportement du LLM** (crÃ©atif, strict, neutre)
- ğŸ“ **Personnaliser les prompts** systÃ¨me et utilisateur
- âš¡ **GÃ©nÃ©rer plusieurs options** de rÃ©ponses
- ğŸ”„ **Surveillance automatique** des fichiers de dialogue

## âœ¨ FonctionnalitÃ©s

### ğŸ­ Gestion Intelligente des Dialogues
- **Surveillance de dossier** : DÃ©tection automatique des nouveaux dialogues
- **SÃ©lection de personnages** : Choix du personnage qui doit rÃ©pondre
- **Analyse contextuelle** : ComprÃ©hension du ton et du style de chaque personnage
- **GÃ©nÃ©ration multiple** : Plusieurs options de rÃ©ponses pour choisir la meilleure

### ğŸ¨ ContrÃ´le du Comportement IA
- **Prompts systÃ¨me personnalisables** : DÃ©finir la personnalitÃ© de l'IA
- **Prompts utilisateur adaptatifs** : ContrÃ´ler le style de rÃ©ponse
- **ParamÃ¨tres fins** : TempÃ©rature, top_p, max_tokens pour un contrÃ´le prÃ©cis
- **ModÃ¨les multiples** : Support de tous les modÃ¨les Ollama

### ğŸ’¬ Interface Chat IntÃ©grÃ©e
- **Chat en temps rÃ©el** avec les modÃ¨les Ollama
- **Historique des conversations** maintenu
- **Test rapide** des prompts et paramÃ¨tres

## ğŸ—ï¸ Justification des Choix Techniques

### Pourquoi Streamlit ?
- âœ… **Prototypage rapide** : Interface web en quelques lignes
- âœ… **RÃ©activitÃ©** : Rechargement automatique lors des modifications
- âœ… **Widgets intuitifs** : Sliders, selectbox, text_area prÃªts Ã  l'emploi
- âœ… **DÃ©ploiement simple** : Un seul fichier `streamlit run app.py`

### Pourquoi Ollama ?
- âœ… **Local et privÃ©** : Pas de donnÃ©es envoyÃ©es vers des API externes
- âœ… **Performance** : ModÃ¨les optimisÃ©s pour le matÃ©riel local
- âœ… **FlexibilitÃ©** : Support de nombreux modÃ¨les (Llama, Mistral, etc.)
- âœ… **CoÃ»t zÃ©ro** : Pas d'abonnement ou de coÃ»ts par token

### Pourquoi Watchdog ?
- âœ… **Surveillance en temps rÃ©el** : DÃ©tection automatique des modifications
- âœ… **EfficacitÃ©** : Pas besoin de polling constant
- âœ… **Cross-platform** : Fonctionne sur Windows, macOS, Linux
- âœ… **IntÃ©gration Streamlit** : Rechargement automatique lors des changements

### Pourquoi TOML pour la configuration ?
- âœ… **LisibilitÃ©** : Format human-friendly
- âœ… **Structure** : Organisation claire des prompts
- âœ… **Ã‰dition facile** : Modification rapide sans redÃ©marrage
- âœ… **Standard** : Format moderne et bien supportÃ©

## ğŸ¬ Cas d'Usage Concrets

### ğŸ“š CrÃ©ation de Contenu
- **ScÃ©naristes** : DÃ©velopper des dialogues naturels entre personnages
- **Auteurs** : Maintenir la cohÃ©rence des voix de personnages dans un roman
- **Game Designers** : CrÃ©er des dialogues de PNJ adaptatifs

### ğŸ¤– DÃ©veloppement de Chatbots
- **EntraÃ®nement de personnalitÃ©s** : Tester diffÃ©rents comportements IA
- **Validation de rÃ©ponses** : GÃ©nÃ©rer plusieurs options pour A/B testing
- **Affinement de prompts** : Optimiser les instructions pour des cas spÃ©cifiques

### ğŸ“ Ã‰ducation et Formation
- **Simulations pÃ©dagogiques** : CrÃ©er des dialogues Ã©ducatifs interactifs
- **Formation en communication** : Pratiquer diffÃ©rents styles conversationnels
- **Analyse comportementale** : Ã‰tudier les patterns de dialogue

## ğŸ”„ Workflow Typique

1. **ğŸ“ PrÃ©paration** : Placer les fichiers de dialogue dans `dialogues_text/`
2. **ğŸ¯ Configuration** : Choisir le modÃ¨le et ajuster les prompts
3. **ğŸ‘¤ SÃ©lection** : Choisir le personnage qui doit rÃ©pondre
4. **ğŸ¨ GÃ©nÃ©ration** : CrÃ©er plusieurs options de rÃ©ponses
5. **âœ… Validation** : SÃ©lectionner la meilleure rÃ©ponse
6. **ğŸ“ IntÃ©gration** : Copier la rÃ©ponse dans le dialogue original

## ğŸ›ï¸ ContrÃ´le Granulaire du LLM

### Niveaux de ContrÃ´le

#### ğŸ—ï¸ **Niveau Architecture** (Prompts SystÃ¨me)
```toml
[system_prompts]
creatif = "Vous Ãªtes un assistant crÃ©atif qui privilÃ©gie l'originalitÃ©"
analytique = "Vous Ãªtes un assistant factuel et mÃ©thodique"
empathique = "Vous Ãªtes un assistant bienveillant et Ã  l'Ã©coute"
```

#### ğŸ¯ **Niveau Comportement** (Prompts Utilisateur)
```toml
[user_prompts]
concis = "RÃ©pondez en une phrase maximum"
detaille = "DÃ©veloppez votre rÃ©ponse avec des exemples"
dramatique = "Ajoutez de la tension Ã©motionnelle"
```

#### âš™ï¸ **Niveau Technique** (ParamÃ¨tres)
- **TempÃ©rature** (0.0-2.0) : CrÃ©ativitÃ© vs CohÃ©rence
- **Top P** (0.0-1.0) : DiversitÃ© du vocabulaire
- **Max Tokens** (10-1000) : Longueur des rÃ©ponses

### Combinaisons RecommandÃ©es

| Cas d'Usage | SystÃ¨me | Utilisateur | TempÃ©rature | Top P |
|-------------|---------|-------------|-------------|--------|
| **Dialogue naturel** | `neutre` | `default` | 0.7 | 0.9 |
| **CrÃ©ativitÃ© littÃ©raire** | `creatif` | `detaille` | 1.2 | 0.95 |
| **Consistance technique** | `strict` | `concis` | 0.3 | 0.8 |
| **Ã‰motion dramatique** | `empathique` | `dramatique` | 0.9 | 0.9 |

## ğŸ› ï¸ PrÃ©requis

### 1. Ollama
```bash
# TÃ©lÃ©charger et installer Ollama depuis https://ollama.ai
# Puis installer au moins un modÃ¨le :
ollama pull llama2-uncensored:latest
ollama pull mistral:7b
```

### 2. Python 3.8+
```bash
python --version  # VÃ©rifier la version
```

## ï¿½ğŸš€ Installation

### 1. Cloner le repository

```bash
git clone https://github.com/votre-username/chatbot-streamlit-ollama.git
cd chatbot_streamlit
```

### 2. CrÃ©er un environnement virtuel (recommandÃ©)

```bash
# CrÃ©er l'environnement
python -m venv .venv

# Activer l'environnement
# Sur Windows :
.venv\Scripts\activate
# Sur macOS/Linux :
source .venv/bin/activate
```

### 3. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

### 4. VÃ©rifier l'installation d'Ollama

```bash
ollama list  # Voir les modÃ¨les installÃ©s
ollama serve  # DÃ©marrer le serveur Ollama (si pas dÃ©jÃ  dÃ©marrÃ©)
```

## ğŸ“± Utilisation

### DÃ©marrage rapide

```bash
# S'assurer qu'Ollama tourne
ollama serve

# Dans un autre terminal, lancer l'app
streamlit run app.py
```

### DÃ©veloppement avec watchdog

Pour le dÃ©veloppement, Streamlit inclut dÃ©jÃ  le rechargement automatique :

```bash
streamlit run app.py --server.runOnSave=true
```

### AccÃ¨s Ã  l'application

- **Local** : <http://localhost:8501>
- **RÃ©seau** : <http://votre-ip:8501>

## ğŸ“ Structure du projet

```
chatbot_streamlit/
â”œâ”€â”€ app.py                 # ğŸ¯ Application Streamlit principale
â”œâ”€â”€ ollama_utils.py        # ğŸ”§ Fonctions utilitaires Ollama
â”œâ”€â”€ prompts.toml          # âš™ï¸ Configuration des prompts
â”œâ”€â”€ requirements.txt      # ğŸ“¦ DÃ©pendances Python
â”œâ”€â”€ dialogues_text/       # ğŸ’¬ Fichiers de dialogue (optionnel)
â”‚   â”œâ”€â”€ dialogue1.txt
â”‚   â”œâ”€â”€ dialogue2.txt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ .gitignore           # ğŸš« Fichiers ignorÃ©s par Git
â”œâ”€â”€ .venv/               # ğŸ Environnement virtuel
â””â”€â”€ README.md            # ğŸ“– Documentation
```

## âš™ï¸ Configuration

### Prompts disponibles

#### ğŸ­ Prompts SystÃ¨me (personnalitÃ© de l'IA)
- **`neutre`** : Assistant utile et neutre
- **`creatif`** : Assistant crÃ©atif et imaginatif  
- **`strict`** : Assistant strict et factuel

#### ğŸ’¬ Prompts Utilisateur (style de rÃ©ponse)
- **`default`** : RÃ©ponses claires et concises
- **`detaille`** : RÃ©ponses dÃ©taillÃ©es et complÃ¨tes
- **`amusant`** : RÃ©ponses amusantes et lÃ©gÃ¨res

### Personnalisation des prompts

Modifiez `prompts.toml` pour ajouter vos propres prompts :

```toml
[system_prompts]
expert = "Vous Ãªtes un expert technique avec une connaissance approfondie."
philosophe = "Vous Ãªtes un philosophe rÃ©flÃ©chi qui explore les idÃ©es profondes."
assistant_code = "Vous Ãªtes un assistant spÃ©cialisÃ© en programmation."

[user_prompts]
educatif = "Expliquez comme si vous enseigniez Ã  un Ã©tudiant dÃ©butant."
professionnel = "RÃ©pondez dans un style professionnel et formel."
technique = "Donnez des rÃ©ponses techniques dÃ©taillÃ©es avec des exemples."
```

### ParamÃ¨tres du modÃ¨le

- **TempÃ©rature** (0.0-2.0) : ContrÃ´le la crÃ©ativitÃ©
  - 0.0 = TrÃ¨s prÃ©visible
  - 1.0 = Ã‰quilibrÃ©
  - 2.0 = TrÃ¨s crÃ©atif

- **Top P** (0.0-1.0) : ContrÃ´le la diversitÃ© du vocabulaire
- **Max Tokens** (10-1000) : Limite la longueur des rÃ©ponses

## ğŸ§ª Surveillance et GÃ©nÃ©ration de Dialogues

### ğŸ“ SystÃ¨me de Surveillance de Dossier

L'application surveille automatiquement le dossier `dialogues_text/` et dÃ©tecte :
- âœ… **Nouveaux fichiers** de dialogue ajoutÃ©s
- âœ… **Modifications** dans les dialogues existants  
- âœ… **Rechargement automatique** de l'interface
- âœ… **Analyse en temps rÃ©el** des personnages prÃ©sents

### ğŸ­ Processus de GÃ©nÃ©ration AssistÃ©e

#### 1. **PrÃ©paration des Dialogues**
Placez vos fichiers `.txt` dans `dialogues_text/` avec le format :
```
Enseignant: Bonjour classe, aujourd'hui on parle de Python.
Ã‰lÃ¨ve: J'ai une question sur les fonctions.
Enseignant: Bien sÃ»r, vas-y.
Ã‰lÃ¨ve: Comment dÃ©finir une fonction ?
```

#### 2. **Analyse Contextuelle Automatique**
L'application analyse automatiquement :
- ğŸ‘¥ **Personnages prÃ©sents** dans le dialogue
- ğŸ¯ **Contexte rÃ©cent** (5 derniÃ¨res interactions par dÃ©faut)
- ğŸ¨ **Ton et style** de chaque personnage
- ğŸ“Š **Patterns conversationnels**

#### 3. **GÃ©nÃ©ration Intelligente**
- **SÃ©lection du personnage** : Choisissez qui doit rÃ©pondre
- **Comportement adaptatif** : L'IA s'adapte au style du personnage
- **Options multiples** : GÃ©nÃ©rez 1 Ã  5 variantes de rÃ©ponse
- **ContrÃ´le fin** : Ajustez prompts et paramÃ¨tres en temps rÃ©el

#### 4. **Workflow d'Assistance**
```
ğŸ“‚ Dossier surveillÃ© â†’ ğŸ” Analyse â†’ ğŸ‘¤ SÃ©lection personnage â†’ ğŸ¨ Configuration â†’ âš¡ GÃ©nÃ©ration â†’ âœ… Validation
```

### ğŸ¯ Types de Comportements Disponibles

#### Prompts SystÃ¨me (PersonnalitÃ© de l'IA)
- **`neutre`** : Assistant Ã©quilibrÃ© pour dialogues naturels
- **`creatif`** : RÃ©ponses imaginatives et originales
- **`strict`** : RÃ©ponses factuelles et cohÃ©rentes
- **`empathique`** : RÃ©ponses bienveillantes et Ã©motionnelles

#### Prompts Utilisateur (Style de RÃ©ponse)
- **`default`** : RÃ©ponses claires et naturelles
- **`detaille`** : DÃ©veloppement approfondi des idÃ©es
- **`amusant`** : Ton lÃ©ger et humoristique
- **`dramatique`** : Tension Ã©motionnelle accrue

### ğŸ”„ Surveillance en Temps RÃ©el

Le systÃ¨me utilise **Watchdog** pour :
- ğŸ“± **DÃ©tection instantanÃ©e** des changements de fichiers
- ğŸ”„ **Rechargement automatique** de l'interface
- âš¡ **Performance optimisÃ©e** (pas de polling)
- ğŸŒ **Cross-platform** (Windows, macOS, Linux)

### ğŸ“Š MÃ©triques et Analyse

L'application fournit :
- ğŸ“ˆ **Statistiques** sur les personnages les plus actifs
- ğŸ¯ **Analyse de cohÃ©rence** des rÃ©ponses gÃ©nÃ©rÃ©es
- ğŸ“ **Historique** des gÃ©nÃ©rations par session
- ğŸ” **Debug visuel** des prompts envoyÃ©s au LLM

### ModÃ¨les recommandÃ©s

```bash
# ModÃ¨les gÃ©nÃ©ralistes
ollama pull llama2:latest          # 7B - Bon Ã©quilibre
ollama pull mistral:latest         # 7B - Rapide et efficace
ollama pull llama2-uncensored      # 7B - Sans filtres

# ModÃ¨les spÃ©cialisÃ©s
ollama pull codellama:latest       # Code et programmation
ollama pull vicuna:latest          # Conversations naturelles

# ModÃ¨les plus puissants (nÃ©cessitent plus de RAM)
ollama pull llama2:13b            # 13B - Plus performant
ollama pull wizard-coder:latest   # SpÃ©cialisÃ© code
```

## ğŸ› ï¸ Technologies utilisÃ©es

| Technologie | Usage | Version |
|-------------|-------|---------|
| **Streamlit** | Interface web interactive | Latest |
| **Ollama** | ModÃ¨les de langage locaux | Latest |
| **Python** | Langage principal | 3.8+ |
| **TOML** | Configuration des prompts | Latest |
| **Watchdog** | Surveillance des fichiers | Latest |

## ğŸš¨ DÃ©pannage

### Erreurs courantes

#### 1. "Impossible de contacter Ollama"
```bash
# VÃ©rifier qu'Ollama tourne
ollama serve

# VÃ©rifier les modÃ¨les installÃ©s
ollama list
```

#### 2. "Port 8501 dÃ©jÃ  utilisÃ©"
```bash
# Utiliser un autre port
streamlit run app.py --server.port=8502
```

#### 3. "Module non trouvÃ©"
```bash
# VÃ©rifier l'environnement virtuel
pip list
pip install -r requirements.txt
```

### Performance

- **RAM recommandÃ©e** : 8GB+ pour les modÃ¨les 7B
- **RAM nÃ©cessaire** : 16GB+ pour les modÃ¨les 13B+
- **CPU** : Plus de cÅ“urs = rÃ©ponses plus rapides

## ğŸ“ DÃ©veloppement

### Structure du code

- `app.py` : Interface utilisateur Streamlit
- `ollama_utils.py` : Logique mÃ©tier et communication Ollama
- `prompts.toml` : Configuration centralisÃ©e

### Ajouter une fonctionnalitÃ©

1. **Fork** le repository
2. **CrÃ©er une branche** : `git checkout -b feature/ma-fonctionnalite`
3. **DÃ©velopper** et tester
4. **Commit** : `git commit -m "âœ¨ Ajout de ma fonctionnalitÃ©"`
5. **Push** : `git push origin feature/ma-fonctionnalite`
6. **Pull Request** sur GitHub

### Tests

```bash
# Lancer l'application en mode dÃ©veloppement
streamlit run app.py --server.runOnSave=true --server.fileWatcherType=watchdog
```

## ï¿½ Licence

MIT License - Voir le fichier LICENSE pour plus de dÃ©tails.

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! 

### Comment contribuer :

1. ğŸ´ **Fork** le projet
2. ğŸŒ¿ **CrÃ©er une branche** pour votre fonctionnalitÃ©
3. âœ… **Tester** vos modifications
4. ğŸ“ **Documenter** les changements
5. ğŸš€ **Soumettre** une Pull Request

### IdÃ©es de contributions :

- ğŸ¨ AmÃ©liorer l'interface utilisateur
- ğŸ”§ Ajouter de nouveaux types de prompts
- ğŸ“Š Ajouter des mÃ©triques et analytics
- ğŸŒ Support multilingue
- ğŸ’¾ Sauvegarde de l'historique des conversations
- ğŸ”Œ IntÃ©gration avec d'autres LLMs

## ğŸ†˜ Support

- ğŸ“§ **Issues** : Ouvrir une issue sur GitHub
- ğŸ’¬ **Discussions** : Section Discussions du repository
- ğŸ“– **Documentation** : Ce README et les commentaires du code

---

â­ **N'oubliez pas de mettre une Ã©toile si ce projet vous est utile !** â­